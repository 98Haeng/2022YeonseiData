{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3강. Advanced Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### key와 value로 데이터를 구분해보자\n",
    "- Spark는 주어진 Rdd의 row가 가지고 있는 첫번째 column을 key로 인식함\n",
    "- 대부분의 Key가 필요한 함수들은 접미어로 Key를 가지고 있음\n",
    " + ex> sortByKey, reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#import findspark; findspark.init()\n",
    "import pyspark\n",
    "\n",
    "#sc = pyspark.SparkContext('local[*]', appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- flatMap(withReplacement, fraction, seed)<br>\n",
    " + 주어진 rdd를 순회하면서 복원/비복원 추출을 주어진 비율만큼 주어진 random seed를 사용하여 추출<br><br>\n",
    "- reduceByKey(withReplacement, fraction, seed)<br>\n",
    " + 주어진 rdd를 key를 기준으로 collect후 주어진 func을 수행<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 1, 3, 1, 2, 2, 4]\n",
      "\n",
      "[(1, 2), (1, 3), (1, 2), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "#중첩된 형태의 rdd를 하나의 row 단위로 풀어 줄 수 있는 명령, 대단히 중요한 명령\n",
    "rdd = sc.parallelize([(1, 2), (1, 3), (1, 2), (2, 4)])\n",
    "print(rdd.flatMap(lambda x:x).collect())\n",
    "print()\n",
    "\n",
    "#중첩된 정도에 따라 row로 풀리는 단위가 다름\n",
    "rdd = sc.parallelize([((1, 2), (1, 3)), ((1, 2), (2, 4))])\n",
    "print(rdd.flatMap(lambda x:x).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', ',', 'b', 'c', ',', 'd', 'e', ',', 'f']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"a,b\", \"c,d\", \"e,f\"])\n",
    "rdd.flatMap(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e', 'f']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda x:x.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 12), (2, 4)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduceByKey를 이용하여 key 별로 reduce를 수행할 수 있음\n",
    "rdd = sc.parallelize([(1, 2), (1, 3), (1, 2), (2, 4), (1, 5)])\n",
    "rdd.reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, <pyspark.resultiterable.ResultIterable object at 0x7fc6fd83a5d0>), (2, <pyspark.resultiterable.ResultIterable object at 0x7fc6fd83a4d0>)]\n",
      "1 [2, 3, 2, 5]\n"
     ]
    }
   ],
   "source": [
    "#groupBy와 유사한 groupByKey동작도 존재함\n",
    "#reduceByKey는 함수로 처리된 하나의 결과값을 남겨두지만\n",
    "#groupByKey는 모든 원소를 보유하고 순회할 수 있음\n",
    "rdd = sc.parallelize([(1, 2), (1, 3), (1, 2), (2, 4), (1, 5)])\n",
    "grouped = rdd.groupByKey()\n",
    "print(grouped.collect())\n",
    "\n",
    "#grouped data중 1이 key인 group 데이터를 추출\n",
    "a, b = grouped.collect()\n",
    "print(a[0], list(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(False, <pyspark.resultiterable.ResultIterable object at 0x7fc6fe148510>), (True, <pyspark.resultiterable.ResultIterable object at 0x7fc6fe148590>)]\n",
      "[1, 3, 5, 7]\n",
      "[2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "#key를 group 기준으로 임의로 만들어 준 뒤 groupByKey나 reduceByKey를 돌리는 것이 일반적\n",
    "#다음 데이터에 짝수, 홀수의 기준으로 key를 분리한뒤 groupByKey로 각 원소를 출력해보자\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "rdd = rdd.map(lambda x:(x % 2 ==0, x))\n",
    "print ( rdd.groupByKey().collect() )\n",
    "\n",
    "a, b = rdd.groupByKey().collect()\n",
    "print (list(a[1]))\n",
    "print (list(b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- mapValues(func)<br>\n",
    " + 주어진 rdd를 순회하면서 key와 value 중 value 부분에 func을 적용<br><br>\n",
    "- flatMapValues(func)<br>\n",
    " + 주어진 rdd를 flat 동작을 value 기준으로 수행하면서 key와 합쳐서 생성해냄<br><br>\n",
    "- groupByKey(func)<br>\n",
    " + rdd의 값을 순회하면서 func가 반환시키는 기준의 데이터로 값들을 묶어주는 동작<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapValues는 key, value로 나눠진 데이터에 대하여 value 쪽만 function으로 컨트롤 가능\n",
    "rdd = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "rdd = rdd.map(lambda x: (x, 0)).mapValues(lambda x:x+1)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', (1, 1)), ('b', (1, 1)), ('c', (1, 1))]\n",
      "[('a', 1), ('a', 1), ('b', 1), ('b', 1), ('c', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "#flatMapValues를 이용하여 key에 대하여 각 value들을 모두 새로운 기준의 row로 분리 가능\n",
    "print( rdd.mapValues(lambda x:(x, x)).collect() )\n",
    "print( rdd.mapValues(lambda x:(x, x)).flatMapValues(lambda x:x).collect() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.391px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
