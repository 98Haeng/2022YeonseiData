{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2강. Basic Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 다양한 종류의 transformation과 action 함수들을 실행해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import findspark; findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello world']\n"
     ]
    }
   ],
   "source": [
    "print(sc.parallelize(['hello world']).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#np array 함수 0-9를 만든 후 출력 해보자\n",
    "import numpy as np\n",
    "foo_np = np.arange(10)\n",
    "print (foo_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- spark.sparkContext.parallelize(c, n)<br>\n",
    " + 주어진 데이터 c를 n개의 rdd로 조각내어 spark에 전달 시키는 명령<br>\n",
    "- rdd.collect()<br>\n",
    " + 주어진 rdd를 하나의 결과로 모으는 명령<br>\n",
    "- rdd.take(n)<br>\n",
    " + 주어진 rdd에서 n개의 데이터를 모으는 명령<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [0, 1, 2]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [0, 1, 2]\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#rdd에 collect함수를 실행하면 action 후 모든 정보를 메모리로 반환시킴 (결과가 대용량 자료일 경우 추천하지 않음)\n",
    "rdd = sc.parallelize(foo_np)\n",
    "print (rdd.collect(), rdd.take(3))\n",
    "rdd = sc.parallelize(foo_np, 10)\n",
    "print (rdd.collect(), rdd.take(3))\n",
    "print (type(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- rdd.map(func)<br>\n",
    " + 주어진 rdd를 순회하면서 func를 통하여 변경된 새로운 rdd를 만드는 명령<br><br>\n",
    "- rdd.filter(func)<br>\n",
    " + 주어진 rdd를 순회하면서 func의 동작이 True인 row를 새로운 rdd로 만드는 명령<br><br>\n",
    "- rdd.first()<br>\n",
    " + take(1)과 동일하나 결과가 row의 배열이 아님<br><br>\n",
    "- rdd.count()<br>\n",
    " + 주어진 rdd의 row 개수를 측정함<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd에서 first 동작을 수행하면 action이 일어나 결과 중 제일 처음 record를 반환함\n",
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd는 map 함수를 주어진 함수에 대하여 전달 할 수 있음\n",
    "#x의 원소가 +5가 되는 rdd를 실행한 뒤 collect를 이용하여 출력해보자\n",
    "rdd.map(lambda x: x + 5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter에 짝수일 경우 참이 되는 function을 전달하여 collect로 출력하여 보자\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rdd.count를 이용하여 짝수개였던 원소가 몇개였는지 출력하여 보자\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- sample(withReplacement, fraction, seed)<br>\n",
    " + 주어진 rdd를 순회하면서 복원/비복원 추출을 주어진 비율만큼 주어진 random seed를 사용하여 추출<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleA: 69\n",
      "sampleB: 45\n",
      "sampleC: 31\n",
      "sampleD: 24\n"
     ]
    }
   ],
   "source": [
    "#값이 0-99인 데이터를 rdd로 변경해보자\n",
    "rdd = sc.parallelize(range(100))\n",
    "\n",
    "#rdd.sample은 원소의 중복을 허용하면서 지정된 비율의 데이터를 표본으로 생성함\n",
    "#sampling은 랜덤함수의 결과로 반환되기 때문에 동일한 명령어여도 값이 다를 수 있음\n",
    "sampleA = rdd.sample(True, 0.5)\n",
    "sampleB = rdd.sample(True, 0.5)\n",
    "print (\"sampleA:\", sampleA.count())\n",
    "print (\"sampleB:\", sampleB.count())\n",
    "\n",
    "#비율이 바뀌면 반환값도 바뀌게 됨\n",
    "sampleC = rdd.sample(True, 0.3)\n",
    "sampleD = rdd.sample(True, 0.3)\n",
    "print (\"sampleC:\", sampleC.count())\n",
    "print (\"sampleD:\", sampleD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0]\n",
      "-------------\n",
      "[1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "#sampleA와 sampleB 결과가 다른 것을 take를 통하여 확인할 수 있음\n",
    "print (sampleA.take(3))\n",
    "print ('-------------')\n",
    "print (sampleB.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampleE: 38\n",
      "sampleF: 38\n",
      "[1, 3, 5]\n",
      "-------------\n",
      "[1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "#랜덤 함수의 seed를 고정하여 동알한 랜덤값이 발생하게 고정 할 수 있음\n",
    "sampleE = rdd.sample(True, 0.3, 1)\n",
    "sampleF = rdd.sample(True, 0.3, 1)\n",
    "print (\"sampleE:\", sampleE.count())\n",
    "print (\"sampleF:\", sampleF.count())\n",
    "print (sampleE.take(3))\n",
    "print ('-------------')\n",
    "print (sampleF.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- union(rdd)<br>\n",
    " + 주어진 rdd와 다른 rdd를 결합함<br><br>\n",
    "- intersection(rdd)<br>\n",
    " + 주어진 rdd와 다른 rdd의 공통적인 row를 추출함<br><br>\n",
    "- distinct([numPartitions])<br>\n",
    " + 주어진 rdd에서 공통적인 row를 제거함<br><br>\n",
    "- cartesian(rdd)<br>\n",
    " + 주어진 2개의 rdd 값을 순회하며서 값들의 조합을 생성함<br><br>\n",
    "- subtract(rdd)<br>\n",
    " + 주어진 2개의 rdd에서 2번째 rdd의 원소들을 제거한 값을 추출함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 4, 1, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#합집합을 수행하여보자\n",
    "a = sc.parallelize([1, 1, 2, 3, 4])\n",
    "b = sc.parallelize([1, 1, 2, 3, 4])\n",
    "\n",
    "ab = a.union(b)\n",
    "print(ab.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "#교집합을 수행하여보자\n",
    "a = sc.parallelize([1, 1, 2, 3, 4, 5, 6, 7])\n",
    "b = sc.parallelize([1, 1, 2, 3, 4])\n",
    "\n",
    "print(a.intersection(b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 2, 3, 3, 4, 2, 4, 1, 5])\n",
    "b = sc.parallelize([1, 2, 2, 3, 3, 4, 1, 5, 2, 6])\n",
    "print(a.intersection(b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 3), (1, 5), (1, 2), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 2), (2, 3), (3, 4), (2, 4), (1, 5)])\n",
    "b = sc.parallelize([(1, 2), (2, 3), (3, 4), (1, 5), (2, 6)])\n",
    "print(a.intersection(b).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 3), (3, 4), (2, 4), (1, 5)]\n",
      "[(1, 5), (1, 2), (2, 4), (2, 3), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "#중복 제거를 수행한뒤 개수를 비교하여 보자\n",
    "print (a.collect())\n",
    "print (a.distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([1, 2, 1, 3, 1, 2, 2, 4, 1, 5])\n",
    "a.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5), (1, 2), (1, 3), (2, 4)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([(1, 2), (1, 3), (1, 2), (2, 4), (1, 5)])\n",
    "a.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'),\n",
       " (1, 'b'),\n",
       " (1, 'c'),\n",
       " (2, 'a'),\n",
       " (2, 'b'),\n",
       " (2, 'c'),\n",
       " (3, 'a'),\n",
       " (3, 'b'),\n",
       " (3, 'c')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-3의 값을 가지는 rdd와 a,b,c 를 가지는 rdd를 생성하여 cartesian product를 수행한 뒤 collect로 출력해보자\n",
    "a = sc.parallelize([1, 2, 3])\n",
    "b = sc.parallelize(['a', 'b', 'c'])\n",
    "a.cartesian(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 5]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-5의 값을 가지는 rdd와 0, 2, 3의 값을 가지는 rdd를 이용하여 subtract를 수행한 뒤 collect로 출력해보자\n",
    "a = sc.parallelize([1, 2, 3, 4, 5])\n",
    "b = sc.parallelize([0, 2, 3])\n",
    "a.subtract(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- zip(withReplacement, fraction, seed)<br>\n",
    " + 서로 다른 2개의 rdd를 인덱스에 맞춰 key, value로 묶어주는 동작<br><br>\n",
    "- groupBy(func)<br>\n",
    " + rdd의 값을 순회하면서 func가 반환시키는 기준의 데이터로 값들을 묶어주는 동작<br><br>\n",
    "- sortBy(keyFunc, ascending, numPartitions)<br>\n",
    " + rdd의 값을 keyFunc의 반환 값 기준으로 ascending의 order로 정렬함 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 2), ('c', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1-3의 값을 가지는 rdd와 a,b,c 를 가지는 rdd를 생성하여 zip을 수행한 뒤 collect로 출력해보자\n",
    "a = sc.parallelize([\"a\", \"b\", \"c\"])\n",
    "b = sc.parallelize([1, 2, 3])\n",
    "a.zip(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'> [(False, <pyspark.resultiterable.ResultIterable object at 0x7f98e4223c50>), (True, <pyspark.resultiterable.ResultIterable object at 0x7f98e4223f90>)]\n",
      "<class 'tuple'> False [1, 3, 5, 7]\n",
      "<class 'tuple'> True [2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "#1-8의 값을 가지는 rdd에 짝수를 판별하는 func를 이용하여 groupby를 수행한 뒤 값을 출력 해보자\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7,8])\n",
    "grouped = rdd.groupBy(lambda x:x%2==0)\n",
    "print (type(grouped), grouped.collect())\n",
    "\n",
    "#grouped의 각 그룹의 결과를 출력하여보자\n",
    "a, b = grouped.collect()\n",
    "print (type(a), a[0], list(a[1]))\n",
    "print (type(b), b[0], list(b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "#1-9의 값을 가진 rdd를 sortBy를 이용하여 데이터를 정렬하여 보자\n",
    "#숫자 타입의 값인 경우 x => -x로 바꿔주는 방식으로 오름차순으로 변경 가능\n",
    "rdd = sc.parallelize(range(10))\n",
    "print ( rdd.sortBy(lambda x:x).collect())\n",
    "print ( rdd.sortBy(lambda x:-x).collect())\n",
    "print ( rdd.sortBy(lambda x:x, False).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- cogroup(rdd)<br>\n",
    " + 서로 다른 rdd를 key 기준으로 value를 묶음<br><br>\n",
    "- join(rdd)<br>\n",
    " + 서로 다른 rdd에서 공통의 key 기준으로 value를 묶음<br><br>\n",
    "- leftOuterJoin(rdd)<br>\n",
    " + 왼쪽 rdd 기준으로 value 를 묶음<br><br>\n",
    "- rightOuterJoin(rdd)<br>\n",
    " + 오른쪽 rdd 기준으로 value 를 묶음<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f98e4268cd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f98e4268f90>)),\n",
       " (2,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f98e4268dd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f98e4268bd0>)),\n",
       " (3,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f98e4268390>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f98e4268b10>)),\n",
       " (4,\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x7f98e4268fd0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x7f98e423add0>))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#key가 존재하는 연산일 경우 cogroup을 사용하여 편리하게 같은 key의 값을 group으로 묶을 수 있다\n",
    "rdd1 = sc.parallelize([(1,1),(1,2),(2,3),(3,4)])\n",
    "rdd2 = sc.parallelize([(1,'a'),(2,'b'),(4,'c'),(1,'d')])\n",
    "cogrouped = rdd1.cogroup(rdd2)\n",
    "cogrouped.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 2] ['a', 'd']\n",
      "2 [3] ['b']\n",
      "3 [4] []\n",
      "4 [] ['c']\n"
     ]
    }
   ],
   "source": [
    "a, b, c, d = cogrouped.collect()\n",
    "print (a[0], list(a[1][0]), list(a[1][1]))\n",
    "print (b[0], list(b[1][0]), list(b[1][1]))\n",
    "print (c[0], list(c[1][0]), list(c[1][1]))\n",
    "print (d[0], list(d[1][0]), list(d[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 'a')), (1, (1, 'd')), (1, (2, 'a')), (1, (2, 'd')), (2, (3, 'b'))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#key가 존재하는 연산일 경우 join을 사용하여 공통 key를 가지고 있는 데이터를 묶어볼 수 있다\n",
    "#공통 key가 존재하지 않는다면 해당 데이터는 제거 됨\n",
    "rdd1.join(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 'a')),\n",
       " (1, (1, 'd')),\n",
       " (1, (2, 'a')),\n",
       " (1, (2, 'd')),\n",
       " (2, (3, 'b')),\n",
       " (3, (4, None))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#공통 key가 없더라도 좌측 기준의 rdd의 key를 기준으로 우측에 rdd가 존재하지 않을 경우 leftOuterJoin를 이용하여 데이터를 묶을 수 있다\n",
    "rdd1.leftOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 'a')),\n",
       " (1, (1, 'd')),\n",
       " (1, (2, 'a')),\n",
       " (1, (2, 'd')),\n",
       " (2, (3, 'b')),\n",
       " (4, (None, 'c'))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leftOuterJoin과 반대로 rightOuterJoin으로 우측기준으로 묶을 수도 있다\n",
    "rdd1.rightOuterJoin(rdd2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- pipe(cmd)<br>\n",
    " + rdd를 순회하면서 cmd 명령의 결과를 전달함<br><br>\n",
    "- reparition(num)<br>\n",
    " + 주어진 rdd를 주어진 num 만큼의 파티션으로 변경<br><br>\n",
    "- coalesce(num)<br>\n",
    " + 주어진 rdd를 주어진 num으로 현재 파티션 개수보다 줄임<br><br>\n",
    "- partitionBy(num, partitionFunc)<br>\n",
    " + 주어진 rdd를 주어진 num의 파티션 만큼 쪼개고 기준은 partitionFunc으로 잡음<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '5', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#linux환경이거나 gow를 깔았다면 유용한 명령어를 spark안에서 수행시킬 수 있음 | 로 연속된 동작은 지원하지 않음\n",
    "rdd = sc.parallelize(str(i) for i in range(20))\n",
    "rdd.pipe('grep [015]').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#병렬 처리 상황 (분산환경, 멀티프로세스 환경)에서 파티션이 많을 경우 executor가 많아져 동시 수행을 할 수 있음\n",
    "rdd = sc.parallelize(range(100))\n",
    "print (rdd.getNumPartitions())\n",
    "rdd = sc.parallelize(range(100), 100)\n",
    "print (rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#repartition의 경우 주어진 rdd의 파티션 개수를 늘이거나 줄일 수 있음\n",
    "rdd = sc.parallelize(range(100))\n",
    "rdd = rdd.repartition(5)\n",
    "print (rdd.getNumPartitions())\n",
    "rdd = rdd.repartition(10)\n",
    "print (rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#coalesce의 경우 주어진 rdd의 파티션 개수를 줄일 수는 있으나 늘릴 수는 없음\n",
    "rdd = sc.parallelize(range(100), 100)\n",
    "rdd = rdd.coalesce(200)\n",
    "print (rdd.getNumPartitions())\n",
    "rdd = rdd.coalesce(10)\n",
    "print (rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#partitionBy의 경우 repartiion과 동일한 연산을 수행하며, partition을 수행할 기준이 되는 func를 넘길 수 있음 (대용량 분산처리에서 간혹  쓰임)\n",
    "rdd = sc.parallelize(range(100), 100)\n",
    "rdd = rdd.partitionBy(3)\n",
    "print (rdd.getNumPartitions())\n",
    "rdd = rdd.partitionBy(10)\n",
    "print (rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- reduce(func)<br>\n",
    " + 주어진 rdd를 순회하면서 기존 값과 현재값의 연산을 func를 통하여 수행 및 반복<br>\n",
    " + 교환법칙과 결합법칙이 적용되는 데이터 타입에만 적용 가능<br><br>\n",
    "- fold(initValue, func)<br>\n",
    " + 주어진 rdd를 순회하면서 기존 값과 현재값의 연산을 func를 통하여 수행 및 반복, 단 초기값 설정 가능<br>\n",
    " + 교환법칙과 결합법칙이 적용되는 데이터 타입에만 적용 가능<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "45\n",
      "\n",
      "3\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "#reduce를 이용하여 값을 합쳐보자\n",
    "rdd = sc.parallelize(range(10))\n",
    "print(rdd.getNumPartitions())\n",
    "print(rdd.reduce(lambda a,b:a+b))\n",
    "print()\n",
    "\n",
    "#파티션이 3개로 주어졌을때와 비교하여 보자\n",
    "rdd = sc.parallelize(range(10), 3)\n",
    "print(rdd.getNumPartitions())\n",
    "print(rdd.reduce(lambda a,b:a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 8\n",
      "54 8\n",
      "\n",
      "45 3\n",
      "49 3\n"
     ]
    }
   ],
   "source": [
    "#fold의 경우로 값을 합쳐보자\n",
    "rdd = sc.parallelize(range(10))\n",
    "print( rdd.fold(0, lambda a,b:a+b), rdd.getNumPartitions())\n",
    "print( rdd.fold(1, lambda a,b:a+b), rdd.getNumPartitions() )\n",
    "print()\n",
    "\n",
    "#fold의 경우 파티션이 나눠진 경우 값이 다른 것을 확인 할 수 있다\n",
    "rdd = sc.parallelize(range(10), 3)\n",
    "print ( rdd.fold(0, lambda a,b:a+b), rdd.getNumPartitions())\n",
    "print ( rdd.fold(1, lambda a,b:a+b), rdd.getNumPartitions() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- foreach(func)<br>\n",
    " + rdd의 값을 순회하면서 func를 실행<br><br>\n",
    "- sum()<br>\n",
    " + rdd의 값을 sum 한뒤 반환<br><br>\n",
    "- min()/max()/mean()<br>\n",
    " + rdd의 값들 중 가장 크거나 작은 값, 평균값을 반환<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#foreach로 원소를 출력하여 보자\n",
    "rdd = sc.parallelize(range(1,10), 3)\n",
    "rdd.foreach(lambda x: print(x))\n",
    "rdd.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "1 9 5.0\n"
     ]
    }
   ],
   "source": [
    "#rdd에 기본 집계 함수를 사용할 수 있음\n",
    "rdd = sc.parallelize(range(1,10), 3)\n",
    "print(rdd.sum())\n",
    "print(rdd.min(), rdd.max(), rdd.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 타이타닉 데이터의 모든 column을 '구별'하여 RDD로 로드하여보자\n",
    " * textFile를 이용하여 로드하시오. #spark SQL이 아닌 spark core의 기능으로만 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Survived,Pclass,Name,Sex,Age,Siblings/Spouses Aboard,Parents/Children Aboard,Fare',\n",
       " '0,3,Mr. Owen Harris Braund,male,22,1,0,7.25',\n",
       " '1,1,Mrs. John Bradley (Florence Briggs Thayer) Cumings,female,38,1,0,71.2833',\n",
       " '1,3,Miss. Laina Heikkinen,female,26,0,0,7.925',\n",
       " '1,1,Mrs. Jacques Heath (Lily May Peel) Futrelle,female,35,1,0,53.1']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('data/titanic.csv')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주어진 타이타닉 사고 데이터를 이용하여 남성/여성의 총 인원수를 집계해보시오"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 주어진 타이타닉 데이터에서 생존 남성의 평균 연령, 생존 여성의 평균 연령을 산출해보시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 타이타닉 사고 데이터에서 남성의 생존률과 여성의 생존률을 Pclass 대비로 측정해 보시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "314.391px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
