{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4강. Spark SQL & Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark; findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"SimpleTest\").getOrCreate()\n",
    "df = spark.sql('''select 'spark' as hello ''')\n",
    "df.show()\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word count를 작성하여 보자\n",
    "1. text파일을 rdd로 변환\n",
    "1. map을 이용하여 word 기준으로 분리\n",
    "1. flatMap을 이용하여 한 단어기준으로 row 추출\n",
    "1. 각 word에 1이라는 count를 value에 부여\n",
    "1. reduce를 실행하여 각 word 별 count를 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-8973a19c150f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-8973a19c150f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    rdd1 = rdd.map( ? )\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#map 부분을 완성해보자\n",
    "rdd = spark.sparkContext.textFile('data/wiki_wordcount.txt')\n",
    "rdd1 = rdd.map( ? )\n",
    "rdd2 = rdd1.flatMap( ? )\n",
    "rdd3 = rdd2.map( ? )\n",
    "rdd3.collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce 부분을 완성한 후 10개를 출력\n",
    "rdd4 = rdd3.reduceByKey( ? )\n",
    "rdd4.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과를 저장\n",
    "rdd4.saveAsTextFile('data/word_count_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) structField와 structType을 이용하여 schema를 부여하고 Dataframe을 만들어 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- spark.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)<br>\n",
    " + data를 dataFrame으로 생성한다 schema가 정의되어 있을때 verifySchema가 True면 invalid 할 시 에러를 냄<br>\n",
    " + pyspark.sql.types 아래의 StructField로 field의 타입 이름, null 값여부를 지정하고<br>\n",
    " + 각 field들을 StructType로 묶어 schema를 만듬<br><br>\n",
    "- df.show(n=20, truncate=True, vertical=False)<br>\n",
    " + df의 데이터를 주어진 n개만큼 출력한다 truncate가 True일 경우 column이 너무 많을 경우 자른다.<br>\n",
    " + vertical이 True일 경우 column의 출력형태를 변경함<br><br>\n",
    "- df.printSchema()<br>\n",
    " + df의 schema 형태를 출력함<br><br>\n",
    "- df.describe(*cols)<br>\n",
    " + 주어진 cols이 숫자형 컬럼이라면 기본통계를 추출함<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+\n",
      "|name|age|country|\n",
      "+----+---+-------+\n",
      "| Son| 34|  Korea|\n",
      "| Kim| 24|  Japan|\n",
      "|Park| 14|  China|\n",
      "+----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r1 = ['Son', 34, 'Korea']\n",
    "r2 = ['Kim', 24, 'Japan']\n",
    "r3 = ['Park', 14, 'China']\n",
    "\n",
    "from pyspark.sql  import types\n",
    "sf1 = types.StructField('name', types.StringType(), True)\n",
    "sf2 = types.StructField('age', types.IntegerType(), True)\n",
    "sf3 = types.StructField('country', types.StringType(), True)\n",
    "\n",
    "schema = types.StructType([sf1, sf2, sf3])\n",
    "rows = [r1, r2, r3]\n",
    "\n",
    "df_first = spark.createDataFrame(rows, schema)\n",
    "df_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_first.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|summary| age|\n",
      "+-------+----+\n",
      "|  count|   3|\n",
      "|   mean|24.0|\n",
      "| stddev|10.0|\n",
      "|    min|  14|\n",
      "|    max|  34|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_first.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- rdd와 마찬가지로 take(n),  count(), collect()같은 기능과 처음 record를 볼 수 있는 head(), first() 등이 존재<br>\n",
    "- df는 pyspark.sql.types.Row로 구성되어져 있음<br>\n",
    "- row는 field name을 이용하여 각 field에 값에 접근 할 수 있음<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Son', age=34, country='Korea')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df_first.head()\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Son'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) word count를 Spark SQL - Dataframe을 이용하여 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Word count',\n",
       " 'From Wikipedia, the free encyclopedia',\n",
       " 'Jump to navigationJump to search',\n",
       " 'The word count is the number of words in a document or passage of text. Word counting may be needed when a text is required to stay within certain numbers of words. This may particularly be the case in academia, legal proceedings, journalism and advertising. Word count is commonly used by translators to determine the price for the translation job. Word counts may also be used to calculate measures of readability and to measure typing and reading speeds (usually in words per minute). When converting character counts to words, a measure of 5 or 6 characters to a word is generally used for English.[1]',\n",
       " '',\n",
       " '',\n",
       " 'Contents',\n",
       " '1\\tDetails and variations of definition',\n",
       " '2\\tSoftware']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile('data/wiki_wordcount.txt')\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- rdd.toDF(schema=None, sampleRatio=None)<br>\n",
    " + rdd는 toDF를 통하여 dataframe으로 간단히 변경 시킬 수 있음\n",
    " + schema나 sampleRatio를 이용하여 변경될 때 데이터 형식, 컬럼 이름, 변경 비율등을 설정 가능<br>\n",
    "- df.groupBy(*cols)<br>\n",
    " + 주어진 컬럼 형태에 따라 groupby를 실행함. df는 reduce가 없기 때문에 groupBy + agg 연산을 결합하는 것이 일반적<br>\n",
    "- df.agg(*cols)<br>\n",
    " + 주어진 groupedData에 agg용 function을 수행함.\n",
    " + from pyspark.sql import functions 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+\n",
      "|          word|cnt|\n",
      "+--------------+---+\n",
      "|          Word|  1|\n",
      "|         count|  1|\n",
      "|          From|  1|\n",
      "|    Wikipedia,|  1|\n",
      "|           the|  1|\n",
      "|          free|  1|\n",
      "|  encyclopedia|  1|\n",
      "|          Jump|  1|\n",
      "|            to|  1|\n",
      "|navigationJump|  1|\n",
      "|            to|  1|\n",
      "|        search|  1|\n",
      "|           The|  1|\n",
      "|          word|  1|\n",
      "|         count|  1|\n",
      "|            is|  1|\n",
      "|           the|  1|\n",
      "|        number|  1|\n",
      "|            of|  1|\n",
      "|         words|  1|\n",
      "+--------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd.flatMap(lambda x:x.split()).map(lambda x:(x,1)).toDF(['word', 'cnt'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----+\n",
      "|summary|             word| cnt|\n",
      "+-------+-----------------+----+\n",
      "|  count|             1276|1276|\n",
      "|   mean|            508.3| 1.0|\n",
      "| stddev|889.1919513332257| 0.0|\n",
      "|    min|          \"Agents|   1|\n",
      "|    max|            فارسی|   1|\n",
      "+-------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- cnt: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   50|\n",
      "|   of|   40|\n",
      "|    a|   36|\n",
      "|  and|   27|\n",
      "|   to|   26|\n",
      "|   is|   22|\n",
      "| word|   22|\n",
      "|   in|   14|\n",
      "|   on|   14|\n",
      "|words|   14|\n",
      "|  for|   12|\n",
      "|  The|   10|\n",
      "| Word|   10|\n",
      "|   as|   10|\n",
      "|count|    9|\n",
      "|  may|    9|\n",
      "|   or|    9|\n",
      "|   be|    8|\n",
      "| also|    6|\n",
      "|   by|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('word').count().orderBy([\"count\", \"word\"], ascending=[0, 1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   50|\n",
      "|   of|   40|\n",
      "|    a|   36|\n",
      "|  and|   27|\n",
      "|   to|   26|\n",
      "|   is|   22|\n",
      "| word|   22|\n",
      "|   in|   14|\n",
      "|   on|   14|\n",
      "|words|   14|\n",
      "|  for|   12|\n",
      "|  The|   10|\n",
      "| Word|   10|\n",
      "|   as|   10|\n",
      "|count|    9|\n",
      "|  may|    9|\n",
      "|   or|    9|\n",
      "|   be|    8|\n",
      "| also|    6|\n",
      "|   by|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql import Column\n",
    "df.groupBy('word').agg(func.count(df.cnt).alias('count')).orderBy([\"count\", \"word\"], ascending=[0, 1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) DataFrameReader/DataFrameWriter를 이용하여 입출력을 수행하여보자\n",
    "- 외부 데이터를 df로 읽거나 쓰기 위해서 read/write 메서드를 지원함\n",
    "- read 순서\n",
    " + read 메서드 호출 -> DataFrameReader 인스턴스 생성\n",
    " + format 메서드를 이용하여 read되는 데이터 소스 유형을 지정 (text, csv, json, jdbc, kafka,parquet, console, socket 등)\n",
    " + option format 별 상세 옵션 지정\n",
    " + load 메서드로 실제 df를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|          Word count|\n",
      "|From Wikipedia, t...|\n",
      "|Jump to navigatio...|\n",
      "|The word count is...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wordcnt = spark.read.format('text').load('data/wiki_wordcount.txt')\n",
    "df_wordcnt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|          Word count|\n",
      "|From Wikipedia, t...|\n",
      "|Jump to navigatio...|\n",
      "|The word count is...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wordcnt = df_wordcnt.withColumnRenamed(\"value\", \"text\")\n",
    "df_wordcnt.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- write 순서\n",
    " + write 메서드 호출 -> DataFrameWriter 인스턴스 생성\n",
    " + format 메서드를 이용하여 read되는 데이터 소스 유형을 지정 (text, csv, json, jdbc, kafka,parquet, console, socket 등)\n",
    " + option format 별 상세 옵션 지정\n",
    " + save 메서드로 실제 파일을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_first.write.format('csv').mode(\"overwrite\").save('data/df_first.csv')\n",
    "df_first.write.format('json').mode(\"overwrite\").save('data/df_first.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- spark.createDataFrame(*cols)<br>\n",
    " + 해당 메서드를 이용하여 pandas dataframe을 바로 spark dataframe으로 변경하는 것도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  k|  v|\n",
      "+---+---+\n",
      "|foo|  1|\n",
      "|bar|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.DataFrame([(\"foo\", 1), (\"bar\", 2)], columns=(\"k\", \"v\"))\n",
    "spark.createDataFrame(df_pd).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) DataFrame의 비타입 연산(untyped operation)을 이용한 word count\n",
    "- dataFrame의 연산을 일반적으로 비타입연산이라고 부름\n",
    "- 비타입연산은 각각의 row에 대하여 다시 지정된 col의 형태로 로직이 처리되는 형태를 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- df.select(*cols)<br>\n",
    " + df에서 지정된 cols를 반환함<br>\n",
    " + 비타입연산 func을 이용하여 col의 value를 변환시킬 수 있음<br>\n",
    " + col은 문자열이외에도 pyspark.sql.functions.col을 이용하여 반환 될 수 있음<br>\n",
    " + pyspark.sql.functions.col은 pyspark.sql.Column을 생성함.<br>\n",
    " + 해당 인스턴스는 실제 col에 대한 정보를 변경 할 수 있음<br>\n",
    "- functions.split(str, sep)<br>\n",
    " + 주어진 문자열을 주어진 sep으로 분리하여 배열 형태로 저장\n",
    "- explode(col)<br>\n",
    " + 주어진 컬럼안의 배열을 각 row로 분리하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|country|age|\n",
      "+-------+---+\n",
      "|  Korea| 34|\n",
      "|  Japan| 24|\n",
      "|  China| 14|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_first.select(['country','age']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|country|age|\n",
      "+-------+---+\n",
      "|  Korea| 34|\n",
      "|  Japan| 24|\n",
      "|  China| 14|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "df_first.select([func.col('country'), func.col('age')]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  cty|age|\n",
      "+-----+---+\n",
      "|Korea| 34|\n",
      "|Japan| 24|\n",
      "|China| 14|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Column\n",
    "df_first.select([func.col('country').alias('cty'), func.col('age')]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      split(_1,  )|\n",
      "+------------------+\n",
      "|[a, b, c, d, e, f]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['a b c d e f']])\n",
    "df.select(func.split(func.col('_1'), ' ')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "|  a|\n",
      "|  b|\n",
      "|  c|\n",
      "|  d|\n",
      "|  e|\n",
      "|  f|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['a b c d e f']])\n",
    "df.select(func.explode(func.split(func.col('_1'), ' '))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_wordcnt 를 이용하여 word count를 Dataframe 명령어로 구현해보자\n",
    "- 정렬은 count 내림 차순 동일한 count를 가진 경우 word 오름차순으로 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|   50|\n",
      "|  of|   40|\n",
      "|   a|   36|\n",
      "| and|   27|\n",
      "|  to|   26|\n",
      "+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df_wordcnt.select(func.explode(func.split(func.col('text'), ' ')).alias('word'))\n",
    "df2 = df1.groupBy('word').count().orderBy(['count', 'word'], ascending=[0, 1])\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Spark SQL을 이용하여 dataframe을 sql을 이용하여 컨트롤 해보자\n",
    "- df는 바로 sql문을 이용하여 처리 될 수 없음\n",
    "- df.createOrReplaceTempView라는 메서드를 통하여 임시의 테이블로 등록된 후 sql로 처리 될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- df.createOrReplaceTempViewe(name)<br>\n",
    " + df를 주어진 name의 임시 테이블로 생성<br>\n",
    " + 해당 테이블은 해당 메서드를 실행한 스파크 세션이 유지되는 동안만 유효함<br>\n",
    "- df.createOrReplaceGlobalTempView(name)<br>\n",
    " + df를 주어진 name의 글로벌 테이블로 생성<br>\n",
    " + 해당 테이블은 전역적인 스파크 세션이 유지되는 동안 유효함<br>\n",
    "- explain()\n",
    " + spark.sql의 실행 계획을 출력함<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('wordcnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|  the|   50|\n",
      "|   of|   40|\n",
      "|    a|   36|\n",
      "|  and|   27|\n",
      "|   to|   26|\n",
      "|     |   25|\n",
      "|   is|   22|\n",
      "| word|   22|\n",
      "|   in|   14|\n",
      "|   on|   14|\n",
      "|words|   14|\n",
      "|  for|   12|\n",
      "|  The|   10|\n",
      "|   as|   10|\n",
      "| Word|    9|\n",
      "|count|    9|\n",
      "|  may|    9|\n",
      "|   or|    9|\n",
      "|   be|    8|\n",
      "| also|    6|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from wordcnt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [count#342L DESC NULLS LAST, word#338 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#342L DESC NULLS LAST, word#338 ASC NULLS FIRST, 200)\n",
      "   +- *(3) HashAggregate(keys=[word#338], functions=[count(1)])\n",
      "      +- Exchange hashpartitioning(word#338, 200)\n",
      "         +- *(2) HashAggregate(keys=[word#338], functions=[partial_count(1)])\n",
      "            +- *(2) Filter (isnotnull(word#338) && (word#338 = words))\n",
      "               +- Generate explode(split(text#269,  )), false, [word#338]\n",
      "                  +- *(1) Project [value#262 AS text#269]\n",
      "                     +- *(1) FileScan text [value#262] Batched: false, Format: Text, Location: InMemoryFileIndex[file:/lecture/진흥원/data/wiki_wordcount.txt], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from wordcnt where word == \"words\"').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|words|   14|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from wordcnt where word == \"words\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) udf (user define function) 을 이용하여 custom function을 수행해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"/>\n",
    "- pyspark.sql.functions.udf(f=None, returnType=StringType)<br>\n",
    " + f에서 수행될 function, returnType에서 pyspark.sql.types의 타입 중 수행후 반환될 타입을 지정히여 udf를 생성할 수 있음<br>\n",
    " + 해당 udf는 dataframe에서는 바로 사용 될 수 있지만, spark.sql 에서는 사용될 수 없음\n",
    "- spark.udf.register(name, f=None, returnType=StringType)<br>\n",
    " + 해당 등록 과정을 통하여 spark.sql안에서도 udf를 사용할 수 있음, 지정된 name으로 지정한 f가 수행 됨\n",
    "- df.withColumn(colName, col)<br>\n",
    " + 지정된 colName의 값들을 col에 지정된 형태로 변경함<br>\n",
    " + colName이 신규이면 새로운 col을 생성함<br>\n",
    "- Column.cast(type)<br>\n",
    " + 주어진 컬럼의 데이터 타입을 type으로 변경함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_sum = func.udf(lambda arr: sum(arr), types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|               val|\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "+------------------+\n",
      "\n",
      "root\n",
      " |-- val: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['1,2,3,4,5,6']])\n",
    "df = df.select(func.split('_1', ',').alias('val'))\n",
    "df = df.withColumn(\"val\", df.val.cast(\"array<integer>\"))\n",
    "df.createOrReplaceTempView('test')\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(arr)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('array_sum', array_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|               val|array_sum(val)|\n",
      "+------------------+--------------+\n",
      "|[1, 2, 3, 4, 5, 6]|            21|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select *, array_sum(val) from test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+\n",
      "|               val|sum_val|\n",
      "+------------------+-------+\n",
      "|[1, 2, 3, 4, 5, 6]|     21|\n",
      "+------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select *, array_sum(val) as sum_val from test').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그외 Spark SQL 명령어는 너무나도 많기 때문에 다음 주소를 참조\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word count를 df와 df 펑션을(spark sql 제외) 이용하여 대문자를 소문자로 전환, 특수문자를 제거 하고 집계 하시오 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* udf와 saprk sql을 이용하여 word count를 대문자를 소문자로 전환 후 특수문자를 제거하고 집계하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
