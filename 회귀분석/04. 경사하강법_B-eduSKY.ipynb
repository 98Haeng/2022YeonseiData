{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###경사하강법:https://www.youtube.com/watch?v=GwR1ivsUiAI\n",
    "gradient descent는 신경망 뿐아니라 전자 공학, 기계 공학 등 다양한 분야에서 사용된다.\n",
    "[출처] 1.5 learning with gradient descent - 1 (경사 하강법)|작성자 박효균\n",
    "https://blog.naver.com/beyondlegend/222161152322\n",
    "\n",
    "# 경사하강법 - 회귀 모델을 구현할 때 최초 회귀 계수를 임의의 값으로 설정한 후 경사하강법을 반복적으로 실행하여 최소의 평균 제곱 오차를 갖는 회귀 계수를 구함\n",
    "\n",
    "손실함수(h)의 어느 한지점에서 h(w)의 미분값과 반대되는 방향으로 w를 조금씩 움직여가며 결과적으로 손실함수(h)의 값이 줄어들게 만드는 공식\n",
    "# 경사하강법을 반복적으로 수행하면 결과적으로 h(w)를 최소로 하는 w를 구하게 됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀식의 계수를 찾는 법 - OLS VS. SGD\n",
    "- 보스턴 집값 데이터 활용(RM VS Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 필요한 라이브러리 import \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "df = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "\n",
    "X = np.array(df[\"RM\"]).reshape(-1, 1)\n",
    "y = boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_score(y_test, y_pred):\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"mse: \",np.round(mse, 3))\n",
    "    print('rmse:', np.round(rmse, 3))\n",
    "    print('r2: ', np.round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LinearRegression 모델을 사용한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.46109164] -30.571032410898336\n",
      "y = 8.461X + (-30.571)\n",
      "mse:  36.517\n",
      "rmse: 6.043\n",
      "r2:  0.602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "#모델 객체 생성\n",
    "ols = LinearRegression()\n",
    "\n",
    "# 모델 학습\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(ols.coef_, ols.intercept_)\n",
    "\n",
    "#회귀식\n",
    "\n",
    "print(\"y = {:.3f}X + ({:.3f})\".format(ols.coef_[0], ols.intercept_))\n",
    "# 예측 수행\n",
    "\n",
    "y_pred = ols.predict(X_test)\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SGDRegressor with hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.98054487] [-2.76201309]\n",
      "y = 3.981X + -2.762\n",
      "mse:  56.636\n",
      "rmse: 7.526\n",
      "r2:  0.382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#모델 객체 생성\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "# 모델 학습\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(sgd.coef_, sgd.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:.3f}X + {:.3f}\".format(sgd.coef_[0], sgd.intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = sgd.predict(X_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SGDRegressor with Scaling: 보스턴집값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.8645052] [22.35825945]\n",
      "y = 5.865X + 22.358\n",
      "mse:  36.49\n",
      "rmse: 6.041\n",
      "r2:  0.602\n"
     ]
    }
   ],
   "source": [
    "# 스케일링(표준화 스케일링)\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train_scaled = (X_train - train_mean) / train_std\n",
    "X_test_scaled = (X_test - train_mean) / train_std\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#모델 객체 생성\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "# 모델 학습\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(sgd.coef_, sgd.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:.3f}X + {:.3f}\".format(sgd.coef_[0], sgd.intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = sgd.predict(X_test_scaled)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.949],\n",
       "       [5.966],\n",
       "       [6.794],\n",
       "       [6.595],\n",
       "       [3.561],\n",
       "       [6.538],\n",
       "       [6.064],\n",
       "       [6.631],\n",
       "       [7.82 ],\n",
       "       [8.78 ],\n",
       "       [6.718],\n",
       "       [6.006],\n",
       "       [5.648],\n",
       "       [6.23 ],\n",
       "       [6.516],\n",
       "       [6.38 ],\n",
       "       [5.693],\n",
       "       [6.302],\n",
       "       [6.513],\n",
       "       [5.783],\n",
       "       [6.047],\n",
       "       [7.079],\n",
       "       [5.713],\n",
       "       [6.245],\n",
       "       [6.782],\n",
       "       [5.88 ],\n",
       "       [7.489],\n",
       "       [5.885],\n",
       "       [6.164],\n",
       "       [6.405],\n",
       "       [6.657],\n",
       "       [7.107],\n",
       "       [5.52 ],\n",
       "       [5.986],\n",
       "       [6.226],\n",
       "       [7.691],\n",
       "       [5.927],\n",
       "       [5.87 ],\n",
       "       [5.889],\n",
       "       [6.525],\n",
       "       [5.856],\n",
       "       [7.313],\n",
       "       [6.03 ],\n",
       "       [6.081],\n",
       "       [6.943],\n",
       "       [5.968],\n",
       "       [7.831],\n",
       "       [5.875],\n",
       "       [6.219],\n",
       "       [4.628],\n",
       "       [6.375],\n",
       "       [6.297],\n",
       "       [5.898],\n",
       "       [6.678],\n",
       "       [7.007],\n",
       "       [6.041],\n",
       "       [6.326],\n",
       "       [5.879],\n",
       "       [5.859],\n",
       "       [6.376],\n",
       "       [6.739],\n",
       "       [6.096],\n",
       "       [5.871],\n",
       "       [6.487],\n",
       "       [5.856],\n",
       "       [5.857],\n",
       "       [6.715],\n",
       "       [6.115],\n",
       "       [6.312],\n",
       "       [5.913],\n",
       "       [6.004],\n",
       "       [5.727],\n",
       "       [5.935],\n",
       "       [7.241],\n",
       "       [6.575],\n",
       "       [6.223],\n",
       "       [5.851],\n",
       "       [6.826],\n",
       "       [6.749],\n",
       "       [5.454],\n",
       "       [6.871],\n",
       "       [5.981],\n",
       "       [6.38 ],\n",
       "       [6.417],\n",
       "       [5.708],\n",
       "       [7.454],\n",
       "       [7.923],\n",
       "       [5.966],\n",
       "       [6.122],\n",
       "       [5.935],\n",
       "       [5.707],\n",
       "       [8.375],\n",
       "       [5.877],\n",
       "       [6.758],\n",
       "       [6.326],\n",
       "       [5.593],\n",
       "       [6.556],\n",
       "       [6.461],\n",
       "       [6.484],\n",
       "       [6.13 ],\n",
       "       [6.854],\n",
       "       [6.425],\n",
       "       [5.349],\n",
       "       [6.939],\n",
       "       [6.436],\n",
       "       [5.757],\n",
       "       [6.616],\n",
       "       [7.088],\n",
       "       [5.706],\n",
       "       [6.546],\n",
       "       [5.304],\n",
       "       [5.803],\n",
       "       [6.552],\n",
       "       [6.852],\n",
       "       [7.686],\n",
       "       [6.727],\n",
       "       [5.617],\n",
       "       [5.998],\n",
       "       [7.163],\n",
       "       [5.427],\n",
       "       [6.794],\n",
       "       [6.129],\n",
       "       [6.438],\n",
       "       [6.066],\n",
       "       [6.24 ],\n",
       "       [6.069],\n",
       "       [5.905],\n",
       "       [6.092],\n",
       "       [5.987],\n",
       "       [5.404],\n",
       "       [6.474],\n",
       "       [6.402],\n",
       "       [5.874],\n",
       "       [5.304],\n",
       "       [6.389],\n",
       "       [5.631],\n",
       "       [5.95 ],\n",
       "       [7.416],\n",
       "       [5.818],\n",
       "       [6.782],\n",
       "       [6.879],\n",
       "       [6.495],\n",
       "       [6.209],\n",
       "       [4.138],\n",
       "       [7.249],\n",
       "       [6.242],\n",
       "       [7.061],\n",
       "       [6.434],\n",
       "       [5.399],\n",
       "       [5.957],\n",
       "       [6.127],\n",
       "       [5.701],\n",
       "       [5.933],\n",
       "       [6.762],\n",
       "       [5.531],\n",
       "       [4.973],\n",
       "       [6.726],\n",
       "       [5.67 ],\n",
       "       [5.682],\n",
       "       [6.982],\n",
       "       [6.315],\n",
       "       [6.373],\n",
       "       [6.655],\n",
       "       [6.549],\n",
       "       [5.965],\n",
       "       [6.433],\n",
       "       [5.972],\n",
       "       [6.975],\n",
       "       [6.454],\n",
       "       [5.924],\n",
       "       [5.604],\n",
       "       [5.891],\n",
       "       [5.757],\n",
       "       [7.041],\n",
       "       [6.176],\n",
       "       [5.807],\n",
       "       [5.936],\n",
       "       [6.208],\n",
       "       [6.406],\n",
       "       [7.412],\n",
       "       [6.642],\n",
       "       [6.383],\n",
       "       [5.952],\n",
       "       [6.343],\n",
       "       [4.906],\n",
       "       [5.627],\n",
       "       [6.144],\n",
       "       [8.069],\n",
       "       [6.254],\n",
       "       [6.482],\n",
       "       [5.872],\n",
       "       [5.536],\n",
       "       [5.875],\n",
       "       [6.152],\n",
       "       [5.888],\n",
       "       [6.02 ],\n",
       "       [6.083],\n",
       "       [6.604],\n",
       "       [7.274],\n",
       "       [6.398],\n",
       "       [6.437],\n",
       "       [7.155],\n",
       "       [4.519],\n",
       "       [5.663],\n",
       "       [6.137],\n",
       "       [6.951],\n",
       "       [6.458],\n",
       "       [6.728],\n",
       "       [6.059],\n",
       "       [4.97 ],\n",
       "       [5.741],\n",
       "       [6.249],\n",
       "       [6.8  ],\n",
       "       [6.174],\n",
       "       [5.186],\n",
       "       [7.61 ],\n",
       "       [5.012],\n",
       "       [5.895],\n",
       "       [6.229],\n",
       "       [6.377],\n",
       "       [5.92 ],\n",
       "       [6.163],\n",
       "       [5.96 ],\n",
       "       [6.897],\n",
       "       [6.816],\n",
       "       [6.59 ],\n",
       "       [4.88 ],\n",
       "       [6.112],\n",
       "       [5.841],\n",
       "       [7.393],\n",
       "       [5.57 ],\n",
       "       [6.459],\n",
       "       [6.031],\n",
       "       [6.286],\n",
       "       [7.135],\n",
       "       [6.14 ],\n",
       "       [7.327],\n",
       "       [5.569],\n",
       "       [5.597],\n",
       "       [5.854],\n",
       "       [7.185],\n",
       "       [6.511],\n",
       "       [8.398],\n",
       "       [8.259],\n",
       "       [6.317],\n",
       "       [6.842],\n",
       "       [6.316],\n",
       "       [6.683],\n",
       "       [6.376],\n",
       "       [6.563],\n",
       "       [5.404],\n",
       "       [6.301],\n",
       "       [6.495],\n",
       "       [6.951],\n",
       "       [6.273],\n",
       "       [6.635],\n",
       "       [6.393],\n",
       "       [5.414],\n",
       "       [6.121],\n",
       "       [6.606],\n",
       "       [5.468],\n",
       "       [6.103],\n",
       "       [7.014],\n",
       "       [6.315],\n",
       "       [5.942],\n",
       "       [7.929],\n",
       "       [6.167],\n",
       "       [5.39 ],\n",
       "       [6.152],\n",
       "       [6.212],\n",
       "       [6.545],\n",
       "       [6.812],\n",
       "       [5.613],\n",
       "       [7.875],\n",
       "       [5.56 ],\n",
       "       [5.759],\n",
       "       [6.167],\n",
       "       [5.709],\n",
       "       [7.645],\n",
       "       [6.051],\n",
       "       [8.04 ],\n",
       "       [3.863],\n",
       "       [5.599],\n",
       "       [7.287],\n",
       "       [7.104],\n",
       "       [5.834],\n",
       "       [5.79 ],\n",
       "       [6.216],\n",
       "       [6.229],\n",
       "       [5.869],\n",
       "       [6.635],\n",
       "       [5.813],\n",
       "       [6.279],\n",
       "       [6.211],\n",
       "       [7.82 ],\n",
       "       [6.998],\n",
       "       [6.209],\n",
       "       [5.602],\n",
       "       [5.683],\n",
       "       [5.   ],\n",
       "       [5.983],\n",
       "       [5.713],\n",
       "       [6.004],\n",
       "       [6.833],\n",
       "       [5.928],\n",
       "       [6.167],\n",
       "       [5.888],\n",
       "       [7.206],\n",
       "       [5.344],\n",
       "       [5.914],\n",
       "       [4.368],\n",
       "       [6.421],\n",
       "       [6.266],\n",
       "       [6.142],\n",
       "       [5.782],\n",
       "       [6.113],\n",
       "       [6.172],\n",
       "       [6.027],\n",
       "       [5.019],\n",
       "       [6.015],\n",
       "       [5.884],\n",
       "       [6.095],\n",
       "       [6.229],\n",
       "       [6.182],\n",
       "       [5.594],\n",
       "       [5.963],\n",
       "       [5.453],\n",
       "       [5.155],\n",
       "       [5.272],\n",
       "       [6.957],\n",
       "       [6.471],\n",
       "       [5.926],\n",
       "       [6.404],\n",
       "       [6.395],\n",
       "       [6.108],\n",
       "       [7.267],\n",
       "       [6.86 ],\n",
       "       [6.968],\n",
       "       [5.713],\n",
       "       [7.358],\n",
       "       [5.961],\n",
       "       [6.701],\n",
       "       [5.637],\n",
       "       [4.903],\n",
       "       [6.185],\n",
       "       [6.037],\n",
       "       [5.822],\n",
       "       [7.853],\n",
       "       [6.193],\n",
       "       [5.876],\n",
       "       [6.065],\n",
       "       [6.405],\n",
       "       [6.086],\n",
       "       [5.85 ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SGD with StadardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.82195706] [22.34514014]\n",
      "y = 5.822X + 22.345\n",
      "mse:  36.613\n",
      "rmse: 6.051\n",
      "r2:  0.601\n"
     ]
    }
   ],
   "source": [
    "#SGD with StandardScaler()\n",
    "# 스케일링(표준화 스케일링)\n",
    "\n",
    "#StandardScaler Code\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#모델 객체 생성\n",
    "sgd = SGDRegressor()\n",
    "\n",
    "# 모델 학습\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(sgd.coef_, sgd.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print(\"y = {:.3f}X + {:.3f}\".format(sgd.coef_[0], sgd.intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = sgd.predict(X_test_scaled)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pipeline with StandardScaler, LinearRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.84750366] [22.31897879]\n",
      "y = 5.847504X + 22.319\n",
      "MSE: 36.523\n",
      "RMSE:  6.043\n",
      "R2:  0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf/lib/python3.9/site-packages/sklearn/linear_model/_stochastic_gradient.py:165: FutureWarning: The loss 'squared_loss' was deprecated in v1.0 and will be removed in version 1.2. Use `loss='squared_error'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "reg = make_pipeline(StandardScaler(),\n",
    "                    SGDRegressor(max_iter=1000000, eta0=0.01,\\\n",
    "                                 tol=0.0001, random_state=42, loss='squared_loss'))\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(reg[1].coef_, reg[1].intercept_)\n",
    "\n",
    "#회귀식 - pipeline()을 사용했기 때문에 SGDRegressor의 parameter가 reg객체의 1번 인덱스에 들어감\n",
    "print(\"y = {:2f}X + {:.3f}\".format(reg[1].coef_[0], reg[1].intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = reg.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# MSE, RMSE, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", np.round(mse, 3))\n",
    "print(\"RMSE: \", np.round(rmse, 3))\n",
    "print(\"R2: \", np.round(r2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################여기까지#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
