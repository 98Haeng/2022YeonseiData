{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###경사하강법:https://www.youtube.com/watch?v=GwR1ivsUiAI\n",
    "gradient descent는 신경망 뿐아니라 전자 공학, 기계 공학 등 다양한 분야에서 사용된다.\n",
    "[출처] 1.5 learning with gradient descent - 1 (경사 하강법)|작성자 박효균\n",
    "https://blog.naver.com/beyondlegend/222161152322\n",
    "\n",
    "# 경사하강법 - 회귀 모델을 구현할 때 최초 회귀 계수를 임의의 값으로 설정한 후 경사하강법을 반복적으로 실행하여 최소의 평균 제곱 오차를 갖는 회귀 계수를 구함\n",
    "\n",
    "손실함수(h)의 어느 한지점에서 h(w)의 미분값과 반대되는 방향으로 w를 조금씩 움직여가며 결과적으로 손실함수(h)의 값이 줄어들게 만드는 공식\n",
    "# 경사하강법을 반복적으로 수행하면 결과적으로 h(w)를 최소로 하는 w를 구하게 됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀식의 계수를 찾는 법 - OLS VS. SGD\n",
    "- 보스턴 집값 데이터 활용(RM VS Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 필요한 라이브러리 import \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 수집 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "df=pd.DataFrame(data=boston.data,columns=boston.feature_names)\n",
    "\n",
    "x= np.array(df['RM']).reshape(-1,1)\n",
    "y= boston.target\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_score(y_test,y_pred):\n",
    "    mse=mean_squared_error(y_test,y_pred)\n",
    "    rmse=np.sqrt(mse)\n",
    "    r2=r2_score(y_test,y_pred)\n",
    "    \n",
    "    print('mse : ',np.round(mse,3))\n",
    "    print('rmse : ',np.round(rmse,3))\n",
    "    print('r2 : ',np.round(r2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LinearRegression 모델을 사용한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.46109164] -30.571032410898336\n",
      "y = 8.461x + -30.571\n",
      "mse :  36.517\n",
      "rmse :  6.043\n",
      "r2 :  0.602\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#모델 객체 생성\n",
    "ols = LinearRegression()\n",
    "\n",
    "# 모델 학습\n",
    "ols.fit(x_train,y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(ols.coef_,ols.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print('y = {:.3f}x + {:.3f}'.format(ols.coef_[0],ols.intercept_))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred=ols.predict(x_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SGDRegressor with hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.4667249] [-4.53246863]\n",
      "y = 4.467x + -4.532\n",
      "mse :  54.046\n",
      "rmse :  7.352\n",
      "r2 :  0.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#모델 객체 생성\n",
    "sgd=SGDRegressor()\n",
    "\n",
    "# 모델 학습\n",
    "sgd.fit(x_train,y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(sgd.coef_,sgd.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print('y = {:.3f}x + {:.3f}'.format(sgd.coef_[0],sgd.intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred=sgd.predict(x_test)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SGDRegressor with Scaling: 보스턴집값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.80646545] [22.32968233]\n",
      "y = 5.806x + 22.330\n",
      "mse :  36.654\n",
      "rmse :  6.054\n",
      "r2 :  0.6\n"
     ]
    }
   ],
   "source": [
    "# 스케일링(표준화 스케일링)\n",
    "train_mean=np.mean(x_train,axis=0)\n",
    "train_std=np.std(x_train,axis=0)\n",
    "\n",
    "x_train_scaled=(x_train - train_mean) / train_std\n",
    "x_test_scaled=(x_test-train_mean) / train_std # 학습 데이터를 변화시킨 것이 기준! 이거를 테스트에도 적용해야됨\n",
    "\n",
    "#모델 객체 생성\n",
    "sgd=SGDRegressor()\n",
    "\n",
    "# 모델 학습\n",
    "sgd.fit(x_train_scaled,y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(sgd.coef_,sgd.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print('y = {:.3f}x + {:.3f}'.format(sgd.coef_[0],sgd.intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred=sgd.predict(x_test_scaled)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SGD with StadardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.87188533] [22.34340748]\n",
      "y = 5.872x + 22.343\n",
      "mse :  36.461\n",
      "rmse :  6.038\n",
      "r2 :  0.602\n"
     ]
    }
   ],
   "source": [
    "#SGD with StandardScaler()\n",
    "# 스케일링(표준화 스케일링)\n",
    "\n",
    "#StandardScaler Code\n",
    "from sklearn.preprocessing import StandardScaler # fit, transform 사용해야함\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "#모델 객체 생성\n",
    "sgd=SGDRegressor()\n",
    "\n",
    "# 모델 학습\n",
    "sgd.fit(x_train_scaled,y_train)\n",
    "\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "print(sgd.coef_,sgd.intercept_)\n",
    "\n",
    "#회귀식\n",
    "print('y = {:.3f}x + {:.3f}'.format(sgd.coef_[0],sgd.intercept_[0]))\n",
    "\n",
    "# 예측 수행\n",
    "y_pred=sgd.predict(x_test_scaled)\n",
    "\n",
    "# MSE, RMSE, r2_score\n",
    "eval_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pipeline with StandardScaler, LinearRegression, SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "reg = make_pipeline(StandardScaler(),\n",
    "                   SGDRegressor(max_iter=100000, eta0=0.01,))\n",
    "# 계수 및 절편 확인: _속성은 학습을 통해 결정되는 속성\n",
    "\n",
    "#회귀식 - pipeline()을 사용했기 때문에 SGDRegressor의 parameter가 reg객체의 1번 인덱스에 들어감\n",
    "\n",
    "\n",
    "# 예측 수행\n",
    "\n",
    "# MSE, RMSE, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################여기까지#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
